{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/models/conda_envs/delius/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config]\n",
      " Qwen2_5_VLConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2_5_VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 128000,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2_5_vl\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1280,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_5_vl\",\n",
      "    \"out_hidden_size\": 2048,\n",
      "    \"spatial_patch_size\": 14,\n",
      "    \"tokens_per_second\": 2\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[processor]\n",
      " None\n",
      "[model]\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor, Qwen2_5_VLConfig\n",
    "model_path=\"/models/Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "config=None\n",
    "processor=None\n",
    "model=None\n",
    "\n",
    "config = Qwen2_5_VLConfig.from_pretrained(model_path)\n",
    "# processor = Qwen2_5_VLProcessor.from_pretrained(model_path)\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path).cuda().eval()\n",
    "print(\"[config]\\n\",config)\n",
    "print(\"[processor]\\n\",processor)\n",
    "print(\"[model]\\n\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"learning cache engine functions and cache engine. No need to initialize\"\n",
    "from typing import *\n",
    "import rich\n",
    "import torch\n",
    "from lmdeploy.pytorch.backends.selector import get_backend\n",
    "from lmdeploy.pytorch.config import BackendConfig, CacheConfig, ModelConfig\n",
    "\n",
    "def __adjust_block_size(model_config:ModelConfig, cache_config: CacheConfig):\n",
    "    \"\"\"\n",
    "    pytorch.engine.model_agent._update_cache_config\n",
    "    adjust block_size.\n",
    "    Currently lmdeploy doesn't support block_size too large.\n",
    "    \"\"\"\n",
    "    # TODO: support kernel with both large head dim and large block size.\n",
    "    if model_config.k_head_dim >= 512 and cache_config.block_size > 32:\n",
    "        cache_config.block_size = 32\n",
    "\n",
    "def _get_key_value_block_shape_impl(\n",
    "        model_config: ModelConfig,\n",
    "        block_size: int,\n",
    "        head_size: int,\n",
    "        world_size: int = 1,\n",
    "        quant_policy: Literal[0, 4, 8] = 0,\n",
    "        local: bool = True\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    get single block shape. stores history tokens predicted by model in single block.\n",
    "    Hence `block_size` you may consider it as max seq_len stored in this block.\n",
    "    Returns:\n",
    "        out(tuple),returns explicit backend shape.\n",
    "        Every device supports different shape.  \n",
    "        For instance,  \n",
    "        cuda supports shape=(block_size, num_heads, head_size);  \n",
    "        ascend supports shape=(block_size, num_heads*head_size)  \n",
    "    \"\"\"\n",
    "    attn_backend = get_backend()\n",
    "    dtype = model_config.dtype\n",
    "    num_heads = model_config.num_key_value_heads\n",
    "    if local:\n",
    "        # k v heads must be divisible by world_size,\n",
    "        # which originally represents num of processes run in all nodes(machines).\n",
    "        # however in lmdeploy it represents `tp` you set.\n",
    "        # tensor parallelism parallelizes grouped k v heads into different devices in one node.\n",
    "        assert num_heads % world_size == 0, \\\n",
    "            f'num_heads: {num_heads}, world_size: {world_size}'\n",
    "        num_heads = num_heads // world_size\n",
    "    if quant_policy == 4:  # pack head_dim to uint8\n",
    "        assert head_size % 2 == 0, \\\n",
    "            f'head_size: {head_size}, quant_policy: {quant_policy}'\n",
    "        head_size = head_size // 2\n",
    "    # get_k_block_shape==get_v_block_shape mostly\n",
    "    return attn_backend.get_k_block_shape(block_size, num_heads, head_size, dtype)\n",
    "\n",
    "def get_cache_block_size(block_size: int,\n",
    "                         model_config: ModelConfig,\n",
    "                         world_size: int = 1,\n",
    "                         quant_policy: int = 0) -> int:\n",
    "    \"\"\"\n",
    "    Get the required cache size of the model in single kvcahce block:\n",
    "    ```\n",
    "    num_layers * (block_size * num_heads * head_size) * 2\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        block_size (int): size of single cache block, used to store token's k,v vector.\n",
    "        model_config (ModelConfig): The config of the model.\n",
    "        quant_policy: 0 represents default float. 4 represents int8 quant.\n",
    "    Returns:\n",
    "       `int`, returns memory size in bytes all key value heads should be allocated.\n",
    "    \"\"\"\n",
    "    num_layers = model_config.num_layers\n",
    "    key_head_size = model_config.k_head_dim\n",
    "    value_head_size = model_config.v_head_dim\n",
    "    if key_head_size is None:\n",
    "        key_head_size = model_config.head_dim\n",
    "    if value_head_size is None:\n",
    "        value_head_size = model_config.head_dim\n",
    "    key_shape = _get_key_value_block_shape_impl(\n",
    "        model_config,\n",
    "        block_size=block_size,\n",
    "        head_size=key_head_size,\n",
    "        world_size=world_size,\n",
    "        local=True,\n",
    "        quant_policy=quant_policy,\n",
    "    ) # (block_size, num_heads, head_size) in cuda; or (block_size, num_heads*head_size) in ascend\n",
    "    value_shape = _get_key_value_block_shape_impl(\n",
    "        model_config,\n",
    "        block_size=block_size,\n",
    "        head_size=value_head_size,\n",
    "        world_size=world_size,\n",
    "        quant_policy=quant_policy,\n",
    "        local=True,\n",
    "    )\n",
    "    if quant_policy == 0:\n",
    "        dtype = model_config.dtype\n",
    "        # meta device represents empty device with no memory allocated.\n",
    "        # tensor on `meta` contains dummy data.\n",
    "        key_block = torch.empty(key_shape, dtype=dtype, device='meta')\n",
    "        value_block = torch.empty(value_shape, dtype=dtype, device='meta')\n",
    "        mem_key_block = key_block.numel() * key_block.element_size() # number of parameters * dtype size.\n",
    "        mem_value_block = value_block.numel() * value_block.element_size()\n",
    "    elif quant_policy in (4, 8):\n",
    "        ...\n",
    "    else:\n",
    "        raise ValueError(f'unsupported quant_policy {quant_policy}')\n",
    "\n",
    "    total = num_layers * (mem_key_block + mem_value_block)\n",
    "    return total\n",
    "\n",
    "def __get_free_gpu_mem_size(model_config:ModelConfig,\n",
    "                            cache_config:CacheConfig,\n",
    "                            cache_block_size:int,\n",
    "                            gpu_id:int)->tuple[int]:\n",
    "    \"\"\"\n",
    "    Get free gpu memory size after prefill.\n",
    "    Args:\n",
    "        cache_block_size(int): memory size in bytes all key value heads should be allocated.\n",
    "    Returns:\n",
    "        out(int): gpu memory free to be allcated subtracted from max runtime memory then multiple `cache_max_entry_count`.\n",
    "    \"\"\"\n",
    "    def __get_runtime_size(num_free_gpu_mem: int,\n",
    "                           cache_block_size: int,\n",
    "                           vocal_size: int):\n",
    "        \"\"\"\n",
    "        find best prefill token num and max runtime size needed.\n",
    "        Estimate max runtime size must be allcated in prefill stage.\n",
    "        Args:\n",
    "            num_free_gpu_mem(int): size of gpu free memory in bytes.\n",
    "        Returns:\n",
    "            out(tuple[int]): `runtime_cache_size`: max gpu memory size in runtime;\n",
    "            `max_prefill_token_num`: update max prefill tokens.\n",
    "        \"\"\"\n",
    "        cache_max_entry_count = cache_config.cache_max_entry_count\n",
    "        max_prefill_token_num = cache_config.max_prefill_token_num # num of tokens accepted to be prefilled.\n",
    "        runtime_cache_size = 0\n",
    "        while max_prefill_token_num > 0:\n",
    "            # lm_head output(2) + to float(4) + estimated misc(1) = 7\n",
    "            \"\"\"\n",
    "            In prefill stage,\n",
    "            hidden_states.shape=[token_num, hidden_dim], dtype=float16 | bfloat16\n",
    "            token_num <= max_prefill_token_num. It multiples lm_head(shape=[tokens_num,vocab_size],dtype=float32)\n",
    "\n",
    "            intermediate size of single parameter at least contains `16bits + 32bits = 6Bytes` and\n",
    "            leave deviation 1Byte which is 7Bytes in total.\n",
    "            \"\"\"\n",
    "            runtime_cache_size = int(max_prefill_token_num * vocal_size * 7) # maximum runtime_cache_size\n",
    "            # (free gpu memory - cache size must be allocated during run time) * cache_max_entry_count \\in [0,1]\n",
    "            num_available = (num_free_gpu_mem - runtime_cache_size) * cache_max_entry_count\n",
    "            if int(num_available) // cache_block_size >= 16:\n",
    "                # 这里有点固定了？ num_available 必须满足 kvcacheblocks 的16倍或以上才break。\n",
    "                # 否则缩减`max_prefill_token_num`为之前的两倍，再计算是否满足16倍。\n",
    "                break\n",
    "            max_prefill_token_num = max_prefill_token_num // 2\n",
    "        return runtime_cache_size, max_prefill_token_num\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gpu_mem_physical_free, _ = torch.cuda.mem_get_info(gpu_id)\n",
    "    vocal_size = model_config.vocab_size\n",
    "\n",
    "    runtime_cache_size, max_prefill_token_num = __get_runtime_size(gpu_mem_physical_free, cache_block_size,\n",
    "                                                                    vocal_size)\n",
    "    if cache_config.max_prefill_token_num != max_prefill_token_num:\n",
    "        if max_prefill_token_num <= 0:\n",
    "            raise RuntimeError('No enough gpu memory for runtime.')\n",
    "        cache_config.max_prefill_token_num = max_prefill_token_num\n",
    "        print(f'device<{gpu_id}> No enough memory. '\n",
    "                        'update max_prefill_token_num='\n",
    "                        f'{max_prefill_token_num}')\n",
    "    gpu_mem_physical_free -= runtime_cache_size\n",
    "    print('estimated max runtime memory:'\n",
    "                    f' {runtime_cache_size>>20} mb')\n",
    "    return gpu_mem_physical_free * cache_config.cache_max_entry_count\n",
    "\n",
    "def _update_cache_config(model_config: ModelConfig,\n",
    "                         cache_config: CacheConfig,\n",
    "                         gpu_id:int = 0,\n",
    "                         host_mem_size:int = 1*(1<<30),\n",
    "                         world_size:int=1):\n",
    "    \"\"\"\n",
    "    Adjust:\n",
    "    --\n",
    "    `cache_config.max_prefill_token_num`: num of maximum prefill token  \n",
    "    `cache_config.num_gpu_blocks`: num of gpu blocks used to store kv cache.  \n",
    "    `cache_config.num_cpu_blocks`: num of cpu blocks  \n",
    "    `cache_config.window_size`: ...\n",
    "    \"\"\"\n",
    "    __adjust_block_size(model_config,cache_config)\n",
    "    cache_block_size = get_cache_block_size(\n",
    "        cache_config.block_size, model_config, world_size,\n",
    "        cache_config.quant_policy\n",
    "    )\n",
    "    gpu_mem = __get_free_gpu_mem_size(model_config,cache_config,cache_block_size,gpu_id)\n",
    "    cpu_mem = host_mem_size\n",
    "    if cache_config.num_cpu_blocks == 0:\n",
    "        cache_config.num_cpu_blocks = int(cpu_mem / cache_block_size)\n",
    "        if cache_config.num_cpu_blocks <= 0:\n",
    "            raise RuntimeError('No enough host memory for kv cache.')\n",
    "    if cache_config.num_gpu_blocks == 0:\n",
    "        cache_config.num_gpu_blocks = int(gpu_mem / cache_block_size)\n",
    "        if cache_config.num_gpu_blocks <= 0:\n",
    "            raise RuntimeError('No enough gpu memory for kv cache.')\n",
    "    cache_config.window_size = model_config.sliding_window\n",
    "\n",
    "    print('block num: {}'.format(cache_config.num_gpu_blocks))\n",
    "    return\n",
    "\n",
    "class CacheEngine:\n",
    "    \"\"\"Host and Device memory maintainer.\n",
    "\n",
    "    Args:\n",
    "        cache_config (CacheConfig): config of the cache information.\n",
    "        model_config (ModelConfig): config of the model.\n",
    "        rank (int): distribution rank, 0 on non-distributed environment.\n",
    "        world_size (int): distribution world size, 1 on non-distributed\n",
    "            environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_config: CacheConfig,\n",
    "        model_config: ModelConfig,\n",
    "        rank: int = 0,\n",
    "        world_size: int = 1,\n",
    "    ) -> None:\n",
    "        if rank == 0:\n",
    "            print(f'build CacheEngine with config:{cache_config}')\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.cache_config = cache_config\n",
    "        self.model_config = model_config\n",
    "\n",
    "        self.block_size = cache_config.block_size\n",
    "        self.num_layers = model_config.num_layers\n",
    "        self.kv_cache_dtype = model_config.dtype\n",
    "        if cache_config.quant_policy > 0:\n",
    "            if self.cache_config.device_type in ['cuda']:\n",
    "                self.kv_cache_dtype = torch.uint8\n",
    "            elif self.cache_config.device_type in ['ascend', 'npu']:\n",
    "                self.kv_cache_dtype = torch.int8\n",
    "            else:\n",
    "                raise ValueError(f'unsupported device_type {self.cache_config.device_type}')\n",
    "\n",
    "        # Initialize the cache.\n",
    "        self.local_gpu_cache = self.allocate_gpu_cache()\n",
    "        self.local_cpu_cache = self.allocate_cpu_cache()\n",
    "\n",
    "        # Initialize the stream for caching operations.\n",
    "        self.cache_stream = torch.cuda.Stream()\n",
    "        assert self.cache_stream != torch.cuda.current_stream()\n",
    "        # Initialize the events for stream synchronization.\n",
    "        self.events = torch.cuda.Event()\n",
    "\n",
    "    @property\n",
    "    def cpu_cache(self):\n",
    "        \"\"\"gpu cache.\"\"\"\n",
    "        return self.local_cpu_cache\n",
    "\n",
    "    @property\n",
    "    def gpu_cache(self):\n",
    "        \"\"\"gpu cache.\"\"\"\n",
    "        return self.local_gpu_cache\n",
    "\n",
    "    @property\n",
    "    def num_gpu_blocks(self):\n",
    "        \"\"\"num gpu blocks.\"\"\"\n",
    "        return self.cache_config.num_gpu_blocks\n",
    "\n",
    "    @property\n",
    "    def num_cpu_blocks(self):\n",
    "        \"\"\"num gpu blocks.\"\"\"\n",
    "        return self.cache_config.num_cpu_blocks\n",
    "\n",
    "    def _allocate_cache(self, num_blocks: int, device: torch.device):\n",
    "        \"\"\"\n",
    "        create and allocate memory cache.\n",
    "        Returns:\n",
    "            out(tuple[torch.Tensor]): `(key_cache, value_cache)`.  \n",
    "            `key_cache.shape=[num_layers, num_blocks, block_size, num_heads, head_size]` if cuda  \n",
    "            `key_cache.shape=[num_layers, num_blocks, block_size, num_heads*head_size]` if ascend  \n",
    "            `value_cache.shape` mostly equal to `key_cache.shape`\n",
    "        \"\"\"\n",
    "        # cuda supports block_shape=(block_size, num_heads, head_size) and\n",
    "        # ascend supports block_shape=(block_size, num_heads*head_size)\n",
    "        key_block_shape = _get_key_value_block_shape_impl(\n",
    "            self.model_config,self.block_size,\n",
    "            head_size=self.model_config.head_dim,\n",
    "            local=True\n",
    "        )\n",
    "        value_block_shape = _get_key_value_block_shape_impl(\n",
    "            self.model_config,self.block_size,\n",
    "            head_size=self.model_config.head_dim,\n",
    "            local=True\n",
    "        )\n",
    "        num_layers = self.num_layers\n",
    "        kv_cache_dtype = self.kv_cache_dtype\n",
    "\n",
    "        key_cache = torch.empty(\n",
    "            size=(num_layers, num_blocks, *key_block_shape),\n",
    "            dtype=kv_cache_dtype,\n",
    "            device=device,\n",
    "        )\n",
    "        value_cache = torch.empty(\n",
    "            size=(num_layers, num_blocks, *value_block_shape),\n",
    "            dtype=kv_cache_dtype,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        output = (key_cache, value_cache)\n",
    "\n",
    "        if self.cache_config.quant_policy in (4, 8):\n",
    "            dtype = self.model_config.dtype\n",
    "            key_sz_cache = torch.empty(\n",
    "                size=(num_layers, num_blocks, *key_block_shape[:-1], 2),\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "            )\n",
    "            val_sz_cache = torch.empty(\n",
    "                size=(num_layers, num_blocks, *value_block_shape[:-1], 2),\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "            )\n",
    "            output = output + (key_sz_cache, val_sz_cache)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def allocate_gpu_cache(self):\n",
    "        \"\"\"allocate caches on GPU.\"\"\"\n",
    "        caches = self._allocate_cache(self.num_gpu_blocks, 'cuda')\n",
    "        self.full_gpu_cache = caches\n",
    "        # self.local_gpu_cache: 第一个维度是`num_layers`，\n",
    "        # 第二个维度是`key_or_value_cache`，\n",
    "        # 后续维度是 single `key_cache` or `value_cache`: [num_blocks, block_size, num_heads, head_size]\n",
    "        self.local_gpu_cache = list(zip(*caches))\n",
    "        return self.local_gpu_cache\n",
    "\n",
    "    def allocate_cpu_cache(self):\n",
    "        \"\"\"allocate caches on Host.\"\"\"\n",
    "        caches = self._allocate_cache(self.num_cpu_blocks, 'cpu')\n",
    "\n",
    "        self.full_cpu_cache = caches\n",
    "        self.local_cpu_cache = list(zip(*caches))\n",
    "        return self.local_cpu_cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"lmdpeloy.[CacheConfig, ModelConfig]\"\n",
    "from lmdeploy import PytorchEngineConfig\n",
    "from lmdeploy.pytorch.config import BackendConfig, CacheConfig, ModelConfig\n",
    "from lmdeploy.pytorch.configurations import AutoModelConfigBuilder\n",
    "\n",
    "engine_config=PytorchEngineConfig()\n",
    "\n",
    "backend_config = BackendConfig(eager_mode=False,device_type='cuda')\n",
    "\n",
    "model_config = AutoModelConfigBuilder.build(\n",
    "    hf_config=config, model_path=model_path, tp=1\n",
    ")\n",
    "model_config.k_head_dim = model_config.v_head_dim = 128\n",
    "# rich.print(\"[ModelConfig]\\n\",model_config)\n",
    "\n",
    "cache_config = CacheConfig(\n",
    "    max_batches=engine_config.max_batch_size,\n",
    "    block_size=engine_config.block_size,\n",
    "    num_cpu_blocks=engine_config.num_cpu_blocks,\n",
    "    num_gpu_blocks=engine_config.num_gpu_blocks,\n",
    "    cache_max_entry_count=engine_config.cache_max_entry_count,\n",
    "    max_prefill_token_num=engine_config.max_prefill_token_num,\n",
    "    enable_prefix_caching=engine_config.enable_prefix_caching,\n",
    "    quant_policy=engine_config.quant_policy,\n",
    "    device_type=engine_config.device_type,\n",
    ")\n",
    "\n",
    "### learning kvcache blocks ###\n",
    "\n",
    "# _update_cache_config(model_config,cache_config)\n",
    "# cache_engine = CacheEngine(cache_config,model_config)\n",
    "\n",
    "### learning kvcache blocks ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- python 中`asyncio.Event()`作用：  \n",
    "用于同步协程各任务的对象。先设置`event = asyncio.Event()`  \n",
    "初始`event.is_set()`为`False`。  \n",
    "各含有`await event.wait()`的协程任务会先在这步暂停，等待`event.is_set()=True`，在通过`event.set()`设置为`True`后，  \n",
    "各协程内的任务就会开始执行。当然你也可以设置多个`Event`用于管理多个协程任务队列。\n",
    "- 例子：\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "import functools\n",
    "  \n",
    "def set_event(event):\n",
    "  print('setting event in callback')\n",
    "  event.set()\n",
    "  \n",
    "async def coro1(event):\n",
    "  print('coro1 waiting for event')\n",
    "  await event.wait()\n",
    "  print('coro1 triggered')\n",
    "  \n",
    "async def coro2(event):\n",
    "  print('coro2 waiting for event')\n",
    "  await event.wait()\n",
    "  print('coro2 triggered')\n",
    "  \n",
    "async def main(loop):\n",
    "  # Create a shared event\n",
    "  event = asyncio.Event()\n",
    "  print('event start state: {}'.format(event.is_set()))\n",
    "  \n",
    "  loop.call_later(\n",
    "    0.1, functools.partial(set_event, event)\n",
    "  )\n",
    "  \n",
    "  await asyncio.wait([coro1(event), coro2(event)])\n",
    "  print('event end state: {}'.format(event.is_set()))\n",
    "  \n",
    "event_loop = asyncio.get_event_loop()\n",
    "try:\n",
    "  event_loop.run_until_complete(main(event_loop))\n",
    "finally:\n",
    "  event_loop.close()\n",
    "```\n",
    "- Output:\n",
    "```shell\n",
    "event start state: False\n",
    "coro2 waiting for event\n",
    "coro1 waiting for event\n",
    "setting event in callback\n",
    "coro2 triggered\n",
    "coro1 triggered\n",
    "event end state: True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lmdeploy总体架构\n",
    "- `lmdeploy.pytorch.engine.Engine`是大脑，`model_agent`，`scheduler`，`request_manger`会在`Engine`中初始化\n",
    "    - 其中`Engine._async_loop`就是整个任务开端。使用3个`asyncio.Event()`相互交错地 await 在各协程任务之间，用于同步各协程任务防止出错\n",
    "        - 协程任务1：`_async_loop_background`，用于模型推理\n",
    "        - 协程任务2：`_async_loop_preprocess_message`，用于预处理输入的`input_ids`与VLM的图像输入，会受到协程任务1的`Event`的限制\n",
    "        - 协程任务3：`_async_send_responses`，用于构造输出，也会受到协程任务1的`Event`的限制\n",
    "\n",
    "- `lmdeploy.pytorch.paging.scheduler.Scheduler`用于调度。\n",
    "    > 开始都是在`waiting`队列。若是`running`小于`max_batches`，就会从`waiting`中取出请求，匹配kvcache缓存表，并置入`running`队列\n",
    "    - `schedule`是其中最主要的调度状态的函数\n",
    "\n",
    "- 模型加载与`CacheEngine`会在`lmdeploy.pytorch.model_agent.*ModelAgent`中初始化\n",
    "    - 各`_build_model`即加载模型的函数\n",
    "    - 各`async_forward`即异步推理函数\n",
    "\n",
    "- `lmdpeloy.pytorch.engine.request.RequestManager`用于管理请求的状态\n",
    "    - 主要函数却在`Engine._bind_request_manager`绑定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-14 08:32:19,373 - lmdeploy - \u001b[33mWARNING\u001b[0m - transformers.py:22 - LMDeploy requires transformers version: [4.33.0 ~ 4.46.1], but found version: 4.49.0.dev0\n",
      "2025-02-14 08:32:19,378 - lmdeploy - \u001b[37mINFO\u001b[0m - model_agent.py:228 - build model.\n",
      "2025-02-14 08:32:19,733 - lmdeploy - \u001b[37mINFO\u001b[0m - model_agent.py:230 - loading weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights from safetensors: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-14 08:32:20,964 - lmdeploy - \u001b[37mINFO\u001b[0m - model_agent.py:232 - loading adapters.\n",
      "2025-02-14 08:32:20,966 - lmdeploy - \u001b[37mINFO\u001b[0m - cache_engine.py:36 - build CacheEngine with config:CacheConfig(max_batches=128, block_size=64, num_cpu_blocks=455, num_gpu_blocks=3852, window_size=-1, cache_max_entry_count=0.8, max_prefill_token_num=4096, enable_prefix_caching=False, quant_policy=0, device_type='cuda')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lmdeploy import Tokenizer\n",
    "from lmdeploy.pytorch.engine import Engine\n",
    "\n",
    "tokenizer = Tokenizer(model_path)\n",
    "engine = Engine(model_path, tokenizer, engine_config, trust_remote_code=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
