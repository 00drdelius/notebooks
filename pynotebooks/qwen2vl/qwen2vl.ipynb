{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/models/conda_envs/delius/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "\"initialize model and preprocessor. attn_implementation==eager\"\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "\n",
    "# executing visual and scatter image features into input_embeds\n",
    "# before set: 13401MiB, after set:5183MiB\n",
    "torch.set_grad_enabled(False)\n",
    "model_dir = \"/models/Qwen/Qwen2-VL-2B-Instruct\"\n",
    "# Load the model in half-precision on the available device(s)\n",
    "model:Qwen2VLForConditionalGeneration = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    ").eval()\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Image Width] 939 [Image Height] 969\n",
      "[image_grid_thw]  tensor([[ 1, 70, 68]], device='cuda:0')\n",
      "[pixel_values shape]  torch.Size([4760, 1176])\n",
      "[input_ids[0,:-15] ]  tensor([151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151653, 151645,    198, 151644,  77091,    198], device='cuda:0')\n",
      "[input_ids shape]  torch.Size([1, 1220])\n"
     ]
    }
   ],
   "source": [
    "\"tokenizer, preprocessor and inputs\"\n",
    "# Image\n",
    "# url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image = Image.open(\"historia.jpg\")\n",
    "print(f\"[Image Width] {image.width} [Image Height] {image.height}\")\n",
    "conversation = [\n",
    "    {\"role\":\"system\",\"content\":\"你是一名高级人工智能助手。\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image in 10 words.\"},\n",
    "            {\"type\": \"image\",\"image\":image},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# print(text_prompt)\n",
    "# Excepted output:\n",
    "# <|im_start|>system\n",
    "# 你是一名高级人工智能助手。<|im_end|>\n",
    "# <|im_start|>user\n",
    "# Describe this image.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
    "# <|im_start|>assistant (if add_generation_prompt)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "# addressed in short:\n",
    "# origin: (width, height): (939, 969)\n",
    "# resize to number can be accurately divided by 28 (28 = merge_size*patch_size: merge_size=2, patch_size=14)\n",
    "# after resize: (952,980) width: round(939/28)*28, height: round(969/28)*28\n",
    "inputs = inputs.to(\"cuda\")\n",
    "# print(inputs.keys())\n",
    "\n",
    "# image_grid_thw: indicates num_of_patches divided by 14 in height and width\n",
    "# [[1,70,68]]: 70=980//14; 68=952//14\n",
    "# image_grid_thw单指patch后width和height的大小，不用算上merge。\n",
    "# 而merge_size用于组合特征：\n",
    "# Qwen2vl 会对所有抽取到的特征,最终按照每相邻的4个patch的特征汇总成一个特征(PatchMerger)\n",
    "print(\"[image_grid_thw] \",inputs.image_grid_thw)\n",
    "\n",
    "# pixel_values.shape=[4760, 1176] 4760=1*70*68=grid_t*grid_h*grid_w\n",
    "# 1176=3*2*14*14=channel*temporal_patch_size*patch_size*patch_size\n",
    "print(\"[pixel_values shape] \",inputs.pixel_values.shape)\n",
    "print(\"[input_ids[0,:-15] ] \",inputs.input_ids[0,-15:])\n",
    "print(\"[input_ids shape] \",inputs.input_ids.shape) #[1,1220]\n",
    "\n",
    "input_ids=inputs.input_ids\n",
    "pixel_values=inputs.pixel_values\n",
    "image_grid_thw=inputs.image_grid_thw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[patches shape]  (2, 3, 8, 8)\n",
      "[patches]\n",
      " [[[[  0   1   2   3   4   5   6   7]\n",
      "   [  8   9  10  11  12  13  14  15]\n",
      "   [ 16  17  18  19  20  21  22  23]\n",
      "   [ 24  25  26  27  28  29  30  31]\n",
      "   [ 32  33  34  35  36  37  38  39]\n",
      "   [ 40  41  42  43  44  45  46  47]\n",
      "   [ 48  49  50  51  52  53  54  55]\n",
      "   [ 56  57  58  59  60  61  62  63]]\n",
      "\n",
      "  [[ 64  65  66  67  68  69  70  71]\n",
      "   [ 72  73  74  75  76  77  78  79]\n",
      "   [ 80  81  82  83  84  85  86  87]\n",
      "   [ 88  89  90  91  92  93  94  95]\n",
      "   [ 96  97  98  99 100 101 102 103]\n",
      "   [104 105 106 107 108 109 110 111]\n",
      "   [112 113 114 115 116 117 118 119]\n",
      "   [120 121 122 123 124 125 126 127]]\n",
      "\n",
      "  [[128 129 130 131 132 133 134 135]\n",
      "   [136 137 138 139 140 141 142 143]\n",
      "   [144 145 146 147 148 149 150 151]\n",
      "   [152 153 154 155 156 157 158 159]\n",
      "   [160 161 162 163 164 165 166 167]\n",
      "   [168 169 170 171 172 173 174 175]\n",
      "   [176 177 178 179 180 181 182 183]\n",
      "   [184 185 186 187 188 189 190 191]]]\n",
      "\n",
      "\n",
      " [[[  0   1   2   3   4   5   6   7]\n",
      "   [  8   9  10  11  12  13  14  15]\n",
      "   [ 16  17  18  19  20  21  22  23]\n",
      "   [ 24  25  26  27  28  29  30  31]\n",
      "   [ 32  33  34  35  36  37  38  39]\n",
      "   [ 40  41  42  43  44  45  46  47]\n",
      "   [ 48  49  50  51  52  53  54  55]\n",
      "   [ 56  57  58  59  60  61  62  63]]\n",
      "\n",
      "  [[ 64  65  66  67  68  69  70  71]\n",
      "   [ 72  73  74  75  76  77  78  79]\n",
      "   [ 80  81  82  83  84  85  86  87]\n",
      "   [ 88  89  90  91  92  93  94  95]\n",
      "   [ 96  97  98  99 100 101 102 103]\n",
      "   [104 105 106 107 108 109 110 111]\n",
      "   [112 113 114 115 116 117 118 119]\n",
      "   [120 121 122 123 124 125 126 127]]\n",
      "\n",
      "  [[128 129 130 131 132 133 134 135]\n",
      "   [136 137 138 139 140 141 142 143]\n",
      "   [144 145 146 147 148 149 150 151]\n",
      "   [152 153 154 155 156 157 158 159]\n",
      "   [160 161 162 163 164 165 166 167]\n",
      "   [168 169 170 171 172 173 174 175]\n",
      "   [176 177 178 179 180 181 182 183]\n",
      "   [184 185 186 187 188 189 190 191]]]]\n",
      "[flatten_patches.T]\n",
      " [[  0   2  16  18   4   6  20  22  32  34  48  50  36  38  52  54]\n",
      " [  1   3  17  19   5   7  21  23  33  35  49  51  37  39  53  55]\n",
      " [  8  10  24  26  12  14  28  30  40  42  56  58  44  46  60  62]\n",
      " [  9  11  25  27  13  15  29  31  41  43  57  59  45  47  61  63]\n",
      " [  0   2  16  18   4   6  20  22  32  34  48  50  36  38  52  54]\n",
      " [  1   3  17  19   5   7  21  23  33  35  49  51  37  39  53  55]\n",
      " [  8  10  24  26  12  14  28  30  40  42  56  58  44  46  60  62]\n",
      " [  9  11  25  27  13  15  29  31  41  43  57  59  45  47  61  63]\n",
      " [ 64  66  80  82  68  70  84  86  96  98 112 114 100 102 116 118]\n",
      " [ 65  67  81  83  69  71  85  87  97  99 113 115 101 103 117 119]\n",
      " [ 72  74  88  90  76  78  92  94 104 106 120 122 108 110 124 126]\n",
      " [ 73  75  89  91  77  79  93  95 105 107 121 123 109 111 125 127]\n",
      " [ 64  66  80  82  68  70  84  86  96  98 112 114 100 102 116 118]\n",
      " [ 65  67  81  83  69  71  85  87  97  99 113 115 101 103 117 119]\n",
      " [ 72  74  88  90  76  78  92  94 104 106 120 122 108 110 124 126]\n",
      " [ 73  75  89  91  77  79  93  95 105 107 121 123 109 111 125 127]\n",
      " [128 130 144 146 132 134 148 150 160 162 176 178 164 166 180 182]\n",
      " [129 131 145 147 133 135 149 151 161 163 177 179 165 167 181 183]\n",
      " [136 138 152 154 140 142 156 158 168 170 184 186 172 174 188 190]\n",
      " [137 139 153 155 141 143 157 159 169 171 185 187 173 175 189 191]\n",
      " [128 130 144 146 132 134 148 150 160 162 176 178 164 166 180 182]\n",
      " [129 131 145 147 133 135 149 151 161 163 177 179 165 167 181 183]\n",
      " [136 138 152 154 140 142 156 158 168 170 184 186 172 174 188 190]\n",
      " [137 139 153 155 141 143 157 159 169 171 185 187 173 175 189 191]]\n",
      "[flatten_patches.T shape]  (24, 16)\n"
     ]
    }
   ],
   "source": [
    "\"analyze Qwen2VLImageProcessor._preprocessor flattens image features\"\n",
    "\"\"\"\n",
    "pixel_values在输入到模型之前已经做好了merge相邻4个patches的预处理。\n",
    "patch: 将[patch_size,patch_size]大小的相邻的像素视作一个patch，\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from easydict import EasyDict\n",
    "self = EasyDict()\n",
    "\n",
    "resized_height=8\n",
    "resized_width=8\n",
    "channel=3\n",
    "self.temporal_patch_size = 2\n",
    "self.merge_size = 2\n",
    "self.patch_size = 2\n",
    "\n",
    "grid_h = resized_height // self.patch_size\n",
    "grid_w = resized_width // self.patch_size\n",
    "patches:np.ndarray = np.arange(\n",
    "    0, 1*channel*resized_height*resized_width\n",
    ").reshape(1,\n",
    "          channel,\n",
    "          resized_height,\n",
    "          resized_width)\n",
    "if patches.shape[0]==1:\n",
    "    patches = np.tile(patches, (self.temporal_patch_size, 1, 1, 1))\n",
    "\n",
    "print(\"[patches shape] \",patches.shape)\n",
    "print(\"[patches]\\n\",patches)\n",
    "\n",
    "channel = patches.shape[1] # 3\n",
    "grid_t = patches.shape[0] // self.temporal_patch_size # 1\n",
    "\n",
    "patches = patches.reshape(\n",
    "    grid_t,\n",
    "    self.temporal_patch_size,\n",
    "    channel,\n",
    "    grid_h // self.merge_size,\n",
    "    self.merge_size,\n",
    "    self.patch_size,\n",
    "    grid_w // self.merge_size,\n",
    "    self.merge_size,\n",
    "    self.patch_size,\n",
    ")\n",
    "patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
    "flatten_patches = patches.reshape(\n",
    "    grid_t * grid_h * grid_w,\n",
    "    channel * self.temporal_patch_size * self.patch_size * self.patch_size\n",
    ")\n",
    "print(\"[flatten_patches.T]\\n\",flatten_patches.T)\n",
    "print(\"[flatten_patches.T shape] \",flatten_patches.T.shape)\n",
    "# flatten_patches.T:\n",
    "# 行元素为每个patch相对位置相同的元素；\n",
    "# 列元素排序优先度：`每个patch的相对位置元素`,再根据`temporal_patch_size`,最后根据`channel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Image Height] 969 [Image Width] 939\n",
      "[Image Resized Height] 980 [Image Resized Width] 952\n",
      "(980, 952, 3)\n",
      "(980, 952, 3)\n"
     ]
    }
   ],
   "source": [
    "\"image smart_resize&rescale&normalize analysis\"\n",
    "import numpy as np\n",
    "from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize, Qwen2VLImageProcessor\n",
    "from transformers.image_transforms import resize\n",
    "\n",
    "img_processor:Qwen2VLImageProcessor = processor.image_processor\n",
    "\n",
    "def temp_save(image:np.ndarray, filename):\n",
    "    Image.fromarray(image,mode='RGB').save(filename,format='jpeg')\n",
    "\n",
    "image = np.array(image)\n",
    "height, width = image.shape[-3],image.shape[-2]\n",
    "print(f\"[Image Height] {height} [Image Width] {width}\")\n",
    "\n",
    "resized_height, resized_width = smart_resize(\n",
    "                    height,\n",
    "                    width,\n",
    "                    factor=img_processor.patch_size * img_processor.merge_size,\n",
    "                    min_pixels=img_processor.min_pixels,\n",
    "                    max_pixels=img_processor.max_pixels,\n",
    "                )\n",
    "print(f\"[Image Resized Height] {resized_height} [Image Resized Width] {resized_width}\")\n",
    "resized_image = resize(image, size=(resized_height, resized_width),\n",
    "                     resample=Image.Resampling.BICUBIC,\n",
    "                     input_data_format=\"channels_last\")\n",
    "print(resized_image.shape)\n",
    "rescale_factor=1\n",
    "rescaled_image=img_processor.rescale(resized_image,rescale_factor,input_data_format=\"channels_last\")\n",
    "print(rescaled_image.shape)\n",
    "# temp_save(rescaled_image,\"test.jpeg\")\n",
    "\n",
    "# omit normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{RoPE formula} \\\\\n",
    "\\text{RoPE会在}Q*K^T\\text{的时候作用。}\\\\\n",
    "\\text{在Q的位置m赋予下面公式，在K的位置n赋予下面公式，两者相乘的值会产生两者的差值} \\\\\n",
    "\\begin{pmatrix}\n",
    "x_{m}^{i} cos m\\theta_{i} - x_{m}^{i+\\frac{d}{2}} sin m\\theta_{i} \\\\\n",
    "x_{m}^{i+\\frac{d}{2}} cos m\\theta_{i} + x_{m}^{i} sin m\\theta_{i}\n",
    "\\end{pmatrix} \\\\\n",
    "d=\\text{hidden dim} \\\\\n",
    "i \\in 1,2,3,...,\\frac{d}{2} \\\\\n",
    "\\text{这里的i即一个token特征(hidden dim)里的元素} \\\\\n",
    "代码中常设的freqs即m\\theta_{i}\n",
    "$$\n",
    "> 详见[RoPE优化与实现](https://nn.labml.ai/transformers/rope/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hpos_ids shape]  torch.Size([70, 68])\n",
      "[hpos_ids]  tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "        [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [67, 67, 67,  ..., 67, 67, 67],\n",
      "        [68, 68, 68,  ..., 68, 68, 68],\n",
      "        [69, 69, 69,  ..., 69, 69, 69]])\n",
      "[hpos_ids shape]  torch.Size([35, 2, 34, 2])\n",
      "[hpos_ids]\n",
      " tensor([[[[ 0,  0],\n",
      "          [ 0,  0],\n",
      "          [ 0,  0],\n",
      "          ...,\n",
      "          [ 0,  0],\n",
      "          [ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 1,  1],\n",
      "          [ 1,  1],\n",
      "          [ 1,  1],\n",
      "          ...,\n",
      "          [ 1,  1],\n",
      "          [ 1,  1],\n",
      "          [ 1,  1]]],\n",
      "\n",
      "\n",
      "        [[[ 2,  2],\n",
      "          [ 2,  2],\n",
      "          [ 2,  2],\n",
      "          ...,\n",
      "          [ 2,  2],\n",
      "          [ 2,  2],\n",
      "          [ 2,  2]],\n",
      "\n",
      "         [[ 3,  3],\n",
      "          [ 3,  3],\n",
      "          [ 3,  3],\n",
      "          ...,\n",
      "          [ 3,  3],\n",
      "          [ 3,  3],\n",
      "          [ 3,  3]]],\n",
      "\n",
      "\n",
      "        [[[ 4,  4],\n",
      "          [ 4,  4],\n",
      "          [ 4,  4],\n",
      "          ...,\n",
      "          [ 4,  4],\n",
      "          [ 4,  4],\n",
      "          [ 4,  4]],\n",
      "\n",
      "         [[ 5,  5],\n",
      "          [ 5,  5],\n",
      "          [ 5,  5],\n",
      "          ...,\n",
      "          [ 5,  5],\n",
      "          [ 5,  5],\n",
      "          [ 5,  5]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[64, 64],\n",
      "          [64, 64],\n",
      "          [64, 64],\n",
      "          ...,\n",
      "          [64, 64],\n",
      "          [64, 64],\n",
      "          [64, 64]],\n",
      "\n",
      "         [[65, 65],\n",
      "          [65, 65],\n",
      "          [65, 65],\n",
      "          ...,\n",
      "          [65, 65],\n",
      "          [65, 65],\n",
      "          [65, 65]]],\n",
      "\n",
      "\n",
      "        [[[66, 66],\n",
      "          [66, 66],\n",
      "          [66, 66],\n",
      "          ...,\n",
      "          [66, 66],\n",
      "          [66, 66],\n",
      "          [66, 66]],\n",
      "\n",
      "         [[67, 67],\n",
      "          [67, 67],\n",
      "          [67, 67],\n",
      "          ...,\n",
      "          [67, 67],\n",
      "          [67, 67],\n",
      "          [67, 67]]],\n",
      "\n",
      "\n",
      "        [[[68, 68],\n",
      "          [68, 68],\n",
      "          [68, 68],\n",
      "          ...,\n",
      "          [68, 68],\n",
      "          [68, 68],\n",
      "          [68, 68]],\n",
      "\n",
      "         [[69, 69],\n",
      "          [69, 69],\n",
      "          [69, 69],\n",
      "          ...,\n",
      "          [69, 69],\n",
      "          [69, 69],\n",
      "          [69, 69]]]])\n",
      "[after permute(0,2,1,3) hpos_ids]\n",
      " tensor([[[[ 0,  0],\n",
      "          [ 1,  1]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 1,  1]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 1,  1]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 1,  1]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 1,  1]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 1,  1]]],\n",
      "\n",
      "\n",
      "        [[[ 2,  2],\n",
      "          [ 3,  3]],\n",
      "\n",
      "         [[ 2,  2],\n",
      "          [ 3,  3]],\n",
      "\n",
      "         [[ 2,  2],\n",
      "          [ 3,  3]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2,  2],\n",
      "          [ 3,  3]],\n",
      "\n",
      "         [[ 2,  2],\n",
      "          [ 3,  3]],\n",
      "\n",
      "         [[ 2,  2],\n",
      "          [ 3,  3]]],\n",
      "\n",
      "\n",
      "        [[[ 4,  4],\n",
      "          [ 5,  5]],\n",
      "\n",
      "         [[ 4,  4],\n",
      "          [ 5,  5]],\n",
      "\n",
      "         [[ 4,  4],\n",
      "          [ 5,  5]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4,  4],\n",
      "          [ 5,  5]],\n",
      "\n",
      "         [[ 4,  4],\n",
      "          [ 5,  5]],\n",
      "\n",
      "         [[ 4,  4],\n",
      "          [ 5,  5]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[64, 64],\n",
      "          [65, 65]],\n",
      "\n",
      "         [[64, 64],\n",
      "          [65, 65]],\n",
      "\n",
      "         [[64, 64],\n",
      "          [65, 65]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[64, 64],\n",
      "          [65, 65]],\n",
      "\n",
      "         [[64, 64],\n",
      "          [65, 65]],\n",
      "\n",
      "         [[64, 64],\n",
      "          [65, 65]]],\n",
      "\n",
      "\n",
      "        [[[66, 66],\n",
      "          [67, 67]],\n",
      "\n",
      "         [[66, 66],\n",
      "          [67, 67]],\n",
      "\n",
      "         [[66, 66],\n",
      "          [67, 67]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[66, 66],\n",
      "          [67, 67]],\n",
      "\n",
      "         [[66, 66],\n",
      "          [67, 67]],\n",
      "\n",
      "         [[66, 66],\n",
      "          [67, 67]]],\n",
      "\n",
      "\n",
      "        [[[68, 68],\n",
      "          [69, 69]],\n",
      "\n",
      "         [[68, 68],\n",
      "          [69, 69]],\n",
      "\n",
      "         [[68, 68],\n",
      "          [69, 69]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[68, 68],\n",
      "          [69, 69]],\n",
      "\n",
      "         [[68, 68],\n",
      "          [69, 69]],\n",
      "\n",
      "         [[68, 68],\n",
      "          [69, 69]]]])\n",
      "[wpos_ids]\n",
      " tensor([[ 0,  1,  2,  ..., 65, 66, 67],\n",
      "        [ 0,  1,  2,  ..., 65, 66, 67],\n",
      "        [ 0,  1,  2,  ..., 65, 66, 67],\n",
      "        ...,\n",
      "        [ 0,  1,  2,  ..., 65, 66, 67],\n",
      "        [ 0,  1,  2,  ..., 65, 66, 67],\n",
      "        [ 0,  1,  2,  ..., 65, 66, 67]])\n",
      "[wpos_ids shape]  torch.Size([4760])\n",
      "[pos_ids shape]  torch.Size([4760, 2])\n",
      "[pos_ids.T[:150,:150]]\n",
      " tensor([[ 0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,\n",
      "          1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,\n",
      "          0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,\n",
      "          1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,\n",
      "          0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,\n",
      "          1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,\n",
      "          0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,\n",
      "          1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  2,  2,  3,  3,  2,  2,  3,  3,\n",
      "          2,  2,  3,  3,  2,  2],\n",
      "        [ 0,  1,  0,  1,  2,  3,  2,  3,  4,  5,  4,  5,  6,  7,  6,  7,  8,  9,\n",
      "          8,  9, 10, 11, 10, 11, 12, 13, 12, 13, 14, 15, 14, 15, 16, 17, 16, 17,\n",
      "         18, 19, 18, 19, 20, 21, 20, 21, 22, 23, 22, 23, 24, 25, 24, 25, 26, 27,\n",
      "         26, 27, 28, 29, 28, 29, 30, 31, 30, 31, 32, 33, 32, 33, 34, 35, 34, 35,\n",
      "         36, 37, 36, 37, 38, 39, 38, 39, 40, 41, 40, 41, 42, 43, 42, 43, 44, 45,\n",
      "         44, 45, 46, 47, 46, 47, 48, 49, 48, 49, 50, 51, 50, 51, 52, 53, 52, 53,\n",
      "         54, 55, 54, 55, 56, 57, 56, 57, 58, 59, 58, 59, 60, 61, 60, 61, 62, 63,\n",
      "         62, 63, 64, 65, 64, 65, 66, 67, 66, 67,  0,  1,  0,  1,  2,  3,  2,  3,\n",
      "          4,  5,  4,  5,  6,  7]])\n",
      "[max_grid_size]  tensor(70, device='cuda:0')\n",
      "[rotary_pos_emb_full shape]  torch.Size([70, 20])\n",
      "[rotary_pos_emb_full[pos_ids] shape]  torch.Size([4760, 2, 20])\n",
      "[rotary_pos_emb shape]  torch.Size([4760, 40])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 6.3096e-01, 3.9811e-01, 2.5119e-01,\n",
      "         1.5849e-01, 1.0000e-01, 6.3096e-02, 3.9811e-02, 2.5119e-02, 1.5849e-02,\n",
      "         1.0000e-02, 6.3096e-03, 3.9811e-03, 2.5119e-03, 1.5849e-03, 1.0000e-03,\n",
      "         6.3096e-04, 3.9811e-04, 2.5119e-04, 1.5849e-04],\n",
      "        [1.0000e+00, 6.3096e-01, 3.9811e-01, 2.5119e-01, 1.5849e-01, 1.0000e-01,\n",
      "         6.3096e-02, 3.9811e-02, 2.5119e-02, 1.5849e-02, 1.0000e-02, 6.3096e-03,\n",
      "         3.9811e-03, 2.5119e-03, 1.5849e-03, 1.0000e-03, 6.3096e-04, 3.9811e-04,\n",
      "         2.5119e-04, 1.5849e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 6.3096e-01, 3.9811e-01, 2.5119e-01, 1.5849e-01, 1.0000e-01,\n",
      "         6.3096e-02, 3.9811e-02, 2.5119e-02, 1.5849e-02, 1.0000e-02, 6.3096e-03,\n",
      "         3.9811e-03, 2.5119e-03, 1.5849e-03, 1.0000e-03, 6.3096e-04, 3.9811e-04,\n",
      "         2.5119e-04, 1.5849e-04, 1.0000e+00, 6.3096e-01, 3.9811e-01, 2.5119e-01,\n",
      "         1.5849e-01, 1.0000e-01, 6.3096e-02, 3.9811e-02, 2.5119e-02, 1.5849e-02,\n",
      "         1.0000e-02, 6.3096e-03, 3.9811e-03, 2.5119e-03, 1.5849e-03, 1.0000e-03,\n",
      "         6.3096e-04, 3.9811e-04, 2.5119e-04, 1.5849e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e+00, 1.2619e+00, 7.9621e-01, 5.0238e-01,\n",
      "         3.1698e-01, 2.0000e-01, 1.2619e-01, 7.9621e-02, 5.0238e-02, 3.1698e-02,\n",
      "         2.0000e-02, 1.2619e-02, 7.9621e-03, 5.0238e-03, 3.1698e-03, 2.0000e-03,\n",
      "         1.2619e-03, 7.9621e-04, 5.0238e-04, 3.1698e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.0000e+00, 1.8929e+00, 1.1943e+00, 7.5357e-01,\n",
      "         4.7547e-01, 3.0000e-01, 1.8929e-01, 1.1943e-01, 7.5357e-02, 4.7547e-02,\n",
      "         3.0000e-02, 1.8929e-02, 1.1943e-02, 7.5357e-03, 4.7547e-03, 3.0000e-03,\n",
      "         1.8929e-03, 1.1943e-03, 7.5357e-04, 4.7547e-04],\n",
      "        [1.0000e+00, 6.3096e-01, 3.9811e-01, 2.5119e-01, 1.5849e-01, 1.0000e-01,\n",
      "         6.3096e-02, 3.9811e-02, 2.5119e-02, 1.5849e-02, 1.0000e-02, 6.3096e-03,\n",
      "         3.9811e-03, 2.5119e-03, 1.5849e-03, 1.0000e-03, 6.3096e-04, 3.9811e-04,\n",
      "         2.5119e-04, 1.5849e-04, 2.0000e+00, 1.2619e+00, 7.9621e-01, 5.0238e-01,\n",
      "         3.1698e-01, 2.0000e-01, 1.2619e-01, 7.9621e-02, 5.0238e-02, 3.1698e-02,\n",
      "         2.0000e-02, 1.2619e-02, 7.9621e-03, 5.0238e-03, 3.1698e-03, 2.0000e-03,\n",
      "         1.2619e-03, 7.9621e-04, 5.0238e-04, 3.1698e-04],\n",
      "        [1.0000e+00, 6.3096e-01, 3.9811e-01, 2.5119e-01, 1.5849e-01, 1.0000e-01,\n",
      "         6.3096e-02, 3.9811e-02, 2.5119e-02, 1.5849e-02, 1.0000e-02, 6.3096e-03,\n",
      "         3.9811e-03, 2.5119e-03, 1.5849e-03, 1.0000e-03, 6.3096e-04, 3.9811e-04,\n",
      "         2.5119e-04, 1.5849e-04, 3.0000e+00, 1.8929e+00, 1.1943e+00, 7.5357e-01,\n",
      "         4.7547e-01, 3.0000e-01, 1.8929e-01, 1.1943e-01, 7.5357e-02, 4.7547e-02,\n",
      "         3.0000e-02, 1.8929e-02, 1.1943e-02, 7.5357e-03, 4.7547e-03, 3.0000e-03,\n",
      "         1.8929e-03, 1.1943e-03, 7.5357e-04, 4.7547e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e+00, 2.5238e+00, 1.5924e+00, 1.0048e+00,\n",
      "         6.3396e-01, 4.0000e-01, 2.5238e-01, 1.5924e-01, 1.0048e-01, 6.3396e-02,\n",
      "         4.0000e-02, 2.5238e-02, 1.5924e-02, 1.0048e-02, 6.3396e-03, 4.0000e-03,\n",
      "         2.5238e-03, 1.5924e-03, 1.0048e-03, 6.3396e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.0000e+00, 3.1548e+00, 1.9905e+00, 1.2559e+00,\n",
      "         7.9245e-01, 5.0000e-01, 3.1548e-01, 1.9905e-01, 1.2559e-01, 7.9245e-02,\n",
      "         5.0000e-02, 3.1548e-02, 1.9905e-02, 1.2559e-02, 7.9245e-03, 5.0000e-03,\n",
      "         3.1548e-03, 1.9905e-03, 1.2559e-03, 7.9245e-04]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n注释：\\n先根据最长的`grid`生成RoPE中的$m\\theta$的编码`rotary_pos_emb_full`，\\n然后用`pos_ids`作为索引从里面抽取对应`height`与`width`的`pos_id`的$m\\theta$编码，\\n再\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"analyze model.visual.rot_pos_emb. This function generates `m\\theta` in RoPE.\"\"\"\n",
    "\"\"\"\n",
    "注意！因为在`preprocessor._preprocess`中输出的图像`pixel_values`已经被处理成了相邻4块一列的形式，\n",
    "因此在`visual`中需要二维width与height的位置信息，而且是非像素而是`patches`为单位\n",
    "\"\"\"\n",
    "pos_ids=[]\n",
    "t,h,w=image_grid_thw[0]\n",
    "\n",
    "hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n",
    "print(\"[hpos_ids shape] \",hpos_ids.shape) # [70,68]\n",
    "print(\"[hpos_ids] \",hpos_ids)\n",
    "hpos_ids = hpos_ids.reshape(\n",
    "        h // model.visual.spatial_merge_size,\n",
    "        model.visual.spatial_merge_size,\n",
    "        w // model.visual.spatial_merge_size,\n",
    "        model.visual.spatial_merge_size,\n",
    "    )\n",
    "print(\"[hpos_ids shape] \",hpos_ids.shape) # [35,2,34,2]\n",
    "print(\"[hpos_ids]\\n\",hpos_ids)\n",
    "hpos_ids=hpos_ids.permute(0,2,1,3)\n",
    "print(\"[after permute(0,2,1,3) hpos_ids]\\n\",hpos_ids)\n",
    "hpos_ids=hpos_ids.flatten() # [4760]\n",
    "\n",
    "# wpos_ids addressed identical as hpos_ids \n",
    "wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n",
    "print(\"[wpos_ids]\\n\",wpos_ids)\n",
    "wpos_ids = wpos_ids.reshape(\n",
    "    h // model.visual.spatial_merge_size,\n",
    "    model.visual.spatial_merge_size,\n",
    "    w // model.visual.spatial_merge_size,\n",
    "    model.visual.spatial_merge_size,\n",
    ")\n",
    "wpos_ids = wpos_ids.permute(0, 2, 1, 3)\n",
    "wpos_ids = wpos_ids.flatten()\n",
    "print(\"[wpos_ids shape] \",wpos_ids.shape) # [4760]\n",
    "\n",
    "pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n",
    "pos_ids = torch.cat(pos_ids, dim=0)\n",
    "print(\"[pos_ids shape] \",pos_ids.shape)\n",
    "print(\"[pos_ids.T[:150,:150]]\\n\",pos_ids.T[:150,:150]) # 可以注意这里的值，`pos_ids.T`，转置后更清楚\n",
    "# print(\"[pos_ids.T[-30:,-30:]]\\n\",pos_ids.T[-30:,-30:])\n",
    "\"\"\"\n",
    "注释：\n",
    "注意上面`pos_ids`：shape=[4760,2],\n",
    "其中`pos_ids[:,0]`为`hpos_ids`，即`height`上的`position_ids`；`pos_ids[:,1]`同理。\n",
    "再注意，这里的位置编码都是以`patch`为单位，\n",
    "【而且在`preprocessor._preprocess`中已经`flatten`为相邻4个`patches`一行的`pixel_values`】\n",
    "因此这的`hpos_ids`与`wpos_ids`都应该4个`patches`地看。\n",
    "如`pos_ids.T`为[[0,0,1,1],[0,1,0,1]]即4个相邻的`patches`。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "max_grid_size = image_grid_thw[:, 1:].max()\n",
    "print(\"[max_grid_size] \",max_grid_size)\n",
    "rotary_pos_emb_full = model.visual.rotary_pos_emb(max_grid_size) #生成$m\\theta$\n",
    "print(\"[rotary_pos_emb_full shape] \",rotary_pos_emb_full.shape) #[70,20]。只在20个特征上填充RoPE\n",
    "\n",
    "\n",
    "#pos_ids的shape是[4760, 2]，这意味着它是一个二维数组，包含4760个索引对，每个索引对由两个整数组成。\n",
    "#rotary_pos_emb_full的shape是[70, 20]，它是一个二维数组，包含70行和20列。\n",
    "#当执行rotary_pos_emb_full[pos_ids]时，PyTorch会使用pos_ids中的每一对索引来从rotary_pos_emb_full中提取相应的元素。\n",
    "#其中每个元素的长度是[1,20]的向量，所以rotary_pos_emb.shape==[4760,2,20]\n",
    "print(\"[rotary_pos_emb_full[pos_ids] shape] \",rotary_pos_emb_full[pos_ids].shape)\n",
    "#由于每一个`patch`具有两个`position id`，因此每一个 patch 会对应到一个 40 维度的向量\n",
    "rotary_pos_emb=rotary_pos_emb_full[pos_ids].flatten(1)\n",
    "print(\"[rotary_pos_emb shape] \",rotary_pos_emb.shape) #[4760,40]\n",
    "print(rotary_pos_emb[:10,:])\n",
    "_=\"\"\"\n",
    "注释：\n",
    "先根据最长的`grid`生成RoPE中的$m\\theta$的编码`rotary_pos_emb_full`，\n",
    "然后用`pos_ids`作为索引从里面抽取对应`height`与`width`的`pos_id`的$m\\theta$编码，\n",
    "再`flatten`，即原本`pos_ids`的[4760,2]`flatten`为[4760,40]，前20个特征是`hpos_ids`的，后20个是`wpos_ids`的。\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_q shape]  torch.Size([1, 4760, 16, 80])\n",
      "[freqs shape]  torch.Size([4760, 40])\n",
      "[cos shape]  torch.Size([1, 4760, 1, 80])\n",
      "[sin shape]==[cos shape]\n",
      "[after_rotate_half shape]  torch.Size([1, 4760, 16, 80])\n",
      "[output shape]  torch.Size([1, 4760, 16, 80])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"analyze apply_rotary_pos_emb_vision in vision attn\"\"\"\n",
    "import torch\n",
    "#assuming input q\n",
    "q=torch.randn([4760,16,80]) #num_heads=16, 80=embed_dim=1280//num_heads\n",
    "input_q=q.unsqueeze(0)\n",
    "freqs:torch.Tensor=rotary_pos_emb\n",
    "print(\"[input_q shape] \",input_q.shape)\n",
    "print(\"[freqs shape] \",freqs.shape)\n",
    "#freqs.cos().unsqueeze(1).shape==[4760,1,40]\n",
    "#repeat(1,1,2):\n",
    "#values on the last dim is repeated at the first 40 and the last 40\n",
    "after_repeat=freqs.cos().unsqueeze(1).repeat(1,1,2) #[4760,1,80]\n",
    "# print(\"[after_repeat shape] \",after_repeat.shape)\n",
    "\n",
    "cos=after_repeat.unsqueeze(0).float().cpu()\n",
    "sin=freqs.sin().unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float().cpu()\n",
    "\"\"\"\n",
    "注意：上面的`freqs`的`cos`与`sin`的`repeat(1,1,2)`即因为`freqs`特征维度40对不上`hidden_states`的80，\n",
    "这里就自行在最后一个特征上`repeat`一倍，`repeat`后的`freqs`可以简化理解为:\n",
    "[`hpos_ids`的$m\\theta$编码,`wpos_ids`的$m\\theta$编码,\n",
    " `hpos_ids`的$m\\theta$编码,`wpos_ids`的$m\\theta$编码]\n",
    "\n",
    "`Q*K^T`的时候：\n",
    "`Q`的m位置与`K`的n位置相乘，对应特征维度的`hpos`与`wpos`会各自点乘，\n",
    "根据RoPE原理，`hpos`会get到`hpos`的相对位置，`wpos`会get到`wpos`的相对位置\n",
    "\"\"\"\n",
    "print(\"[cos shape] \",cos.shape)\n",
    "print(\"[sin shape]==[cos shape]\")\n",
    "\n",
    "### rotate_half ###\n",
    "x1=input_q[..., :input_q.shape[-1] // 2]\n",
    "x2=input_q[..., input_q.shape[-1] // 2:]\n",
    "# 前`d/2`放后面且置为正，后`d/2`放前面且置为负\n",
    "after_rotate_half=torch.cat((-x2, x1), dim=-1)\n",
    "print(\"[after_rotate_half shape] \", after_rotate_half.shape)\n",
    "### rotate_half ###\n",
    "\n",
    "#简略见上markdown注释\n",
    "#优化思路：只需要在`Q*K^T`的时候满足m位置的q与n位置的k相乘有相对位置信息即可。\n",
    "#将信息嵌入到前x个hidden_dim也能满足位置信息存在于`Q*K^T`后的分数矩阵：\n",
    "#因此原RoPE的嵌入所有hidden_dim，优化为嵌入自己设置的前x个hidden_dim就行。\n",
    "#当然也可以全选，这里就全选了分注意力头之后的hidden_dim=80，全部嵌入位置信息\n",
    "\n",
    "# 注意这里 `*` 是按位乘\n",
    "output=(input_q * cos)+(after_rotate_half * sin)\n",
    "print(\"[output shape] \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pixel_values shape]  torch.Size([4760, 1176])\n",
      "[after PatchEmbed]  torch.Size([4760, 1280])\n",
      "[grid_thw]  tensor([[ 1, 70, 68]], device='cuda:0')\n",
      "[rotary_pos_emb shape]  torch.Size([4760, 40])\n",
      "[grid_thw[:,1]]  tensor([70], device='cuda:0')\n",
      "[grid_thw[:,2]]  tensor([68], device='cuda:0')\n",
      "[grid_thw[:,0]]  tensor([1], device='cuda:0')\n",
      "[after repeat_interleave]  tensor([4760], device='cuda:0')\n",
      "[cu_seqlens]  tensor([   0, 4760], device='cuda:0', dtype=torch.int32)\n",
      "## VisionAttention ##\n",
      "[hidden_states]  torch.Size([4760, 1280])\n",
      "[attention_mask]\n",
      " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "[attn_output shape]  torch.Size([4760, 1280])\n",
      "[vision_mlp_output shape]  torch.Size([4760, 1280])\n",
      "[after_merger shape]  torch.Size([1190, 1536])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"analyze vision model\n",
    "image_embeds = model.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "image_embeds.shape=[1190, 1536]\n",
    "\"\"\"\n",
    "# omit RMSNorm\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.qwen2_vl.modeling_qwen2_vl import apply_rotary_pos_emb_vision\n",
    "print(\"[pixel_values shape] \",pixel_values.shape)\n",
    "hidden_states = model.visual.patch_embed(pixel_values)\n",
    "print(\"[after PatchEmbed] \", hidden_states.shape)\n",
    "grid_thw=image_grid_thw\n",
    "print(\"[grid_thw] \",grid_thw)\n",
    "\n",
    "# please see beneath analysis if has any question\n",
    "rotary_pos_emb = model.visual.rot_pos_emb(grid_thw)\n",
    "print(\"[rotary_pos_emb shape] \",rotary_pos_emb.shape)\n",
    "print(\"[grid_thw[:,1]] \",grid_thw[:,1])\n",
    "print(\"[grid_thw[:,2]] \",grid_thw[:,2])\n",
    "print(\"[grid_thw[:,0]] \",grid_thw[:,0])\n",
    "\n",
    "cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n",
    "    dim=0, dtype=torch.int32\n",
    ")\n",
    "print(\"[after repeat_interleave] \",torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]))\n",
    "cu_seqlens = F.pad(cu_seqlens, (1,0), value=0)\n",
    "print(\"[cu_seqlens] \",cu_seqlens)\n",
    "\n",
    "# VisionBlock: residual with attn and mlp.\n",
    "# omit RMSNorm\n",
    "## VisionAttention\n",
    "print(\"## VisionAttention ##\")\n",
    "seq_length = hidden_states.shape[0]\n",
    "print(\"[hidden_states] \",hidden_states.shape) #[4760,1280]\n",
    "after_qkv=model.visual.blocks[0].attn.qkv(hidden_states) #[4760,3840]\n",
    "num_heads=model.config.vision_config.num_heads #16\n",
    "qkv_reshaped=after_qkv.reshape(seq_length,3,num_heads,-1) #[4760,3,16,80]\n",
    "_=qkv_reshaped.permute(1,0,2,3) #[3,4760,16,80]\n",
    "#unbind,turns given dim to tuple\n",
    "q,k,v=_.unbind(0) #q.shape==k.shape==v.shape==[4760,16,80]\n",
    "q=apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n",
    "k=apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n",
    "\n",
    "attention_mask = torch.full(\n",
    "    [1, seq_length, seq_length], torch.finfo(q.dtype).min, device=q.device, dtype=q.dtype\n",
    ")\n",
    "for i in range(1, len(cu_seqlens)):\n",
    "    attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n",
    "print(\"[attention_mask]\\n\",attention_mask)\n",
    "\n",
    "attn_head_dim=model.visual.blocks[0].attn.head_dim\n",
    "attn_proj=model.visual.blocks[0].attn.proj\n",
    "\n",
    "q = q.transpose(0, 1)\n",
    "k = k.transpose(0, 1)\n",
    "v = v.transpose(0, 1)\n",
    "\n",
    "attn_weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(attn_head_dim)\n",
    "attn_weights = attn_weights + attention_mask\n",
    "attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n",
    "attn_output = torch.matmul(attn_weights, v)\n",
    "attn_output = attn_output.transpose(0, 1)\n",
    "attn_output = attn_output.reshape(seq_length, -1)\n",
    "attn_output = attn_proj(attn_output)\n",
    "\n",
    "print(\"[attn_output shape] \",attn_output.shape)\n",
    "\n",
    "### Vision MLP ###\n",
    "fc1 = model.visual.blocks[0].mlp.fc1 # in=1280,out=5120==1280*4\n",
    "vision_act_fn = model.visual.blocks[0].mlp.act #QuickGELUActivation\n",
    "fc2 = model.visual.blocks[0].mlp.fc2 # in=5120,out=1280\n",
    "vision_mlp_output=fc2(vision_act_fn(fc1(attn_output)))\n",
    "### Vision MLP ###\n",
    "print(\"[vision_mlp_output shape] \",vision_mlp_output.shape)\n",
    "\n",
    "### Merger ###\n",
    "# function: merge 4 patches and project to be embeded into text input embeds\n",
    "merger=model.visual.merger\n",
    "hidden_size = merger.hidden_size #5120==1280*2**2 (spatial_merge_size==2)\n",
    "after_merger=merger(vision_mlp_output)\n",
    "print(\"[after_merger shape] \",after_merger.shape)\n",
    "# [4760,1280] (after view) -> [1190,5120] (after merger) -> [1190,1536]\n",
    "# 4760=70*68( width*height ), merge_size==2\n",
    "# Qwen2VL merge 4 spatially adjacent patches, which has been ordered in preprocessor.preprocess\n",
    "### Merger ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inputs_embeds shape] torch.Size([1, 1220, 1536])\n",
      "[image_embeds shape] torch.Size([1190, 1536])\n",
      "[n_image_tokens]  1190  [n_image_features]  1190\n",
      "[temp1 shape]  torch.Size([1, 1220]) \n",
      "[temp1[0,-15:]]  tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "        False, False, False, False, False], device='cuda:0')\n",
      "[temp2 shape]  torch.Size([1, 1220, 1536]) \n",
      "[temp2[0,-15:,0]]  tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "        False, False, False, False, False], device='cuda:0')\n",
      "[image_mask shape]  torch.Size([1, 1220, 1536])\n",
      "tensor([-0.0101, -0.0101, -0.0101, -0.0101, -0.0101, -0.0101, -0.0101, -0.0101,\n",
      "        -0.0101, -0.0101, -0.0076,  0.0330, -0.0074,  0.0114,  0.0330],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([-0.7930, -0.0942, -0.4668, -0.4375, -1.5859, -0.2158, -0.6523, -1.4922,\n",
      "        -4.3750, -0.0101, -0.0076,  0.0330, -0.0074,  0.0114,  0.0330],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"analyze image mask and generate inputs_embeds\"\"\"\n",
    "\"\"\"\n",
    "Simply speaking. image_embeds is directly embeded into inputs_embeds.\n",
    "Just inplace all token_ids==151655. The tokens length has to be equal to image_embeds\n",
    "or it loses features by function `masked_scatter`.\n",
    "\"\"\"\n",
    "image_token_id=151655\n",
    "inputs_embeds = model.model.embed_tokens(input_ids) #[1,1220,1536]\n",
    "image_embeds = model.visual(pixel_values, grid_thw=image_grid_thw) #[1190, 1536]\n",
    "print('[inputs_embeds shape]', inputs_embeds.shape)\n",
    "print('[image_embeds shape]',image_embeds.shape)\n",
    "\n",
    "n_image_tokens = (input_ids == model.config.image_token_id).sum().item() #number of image tokens 1190\n",
    "n_image_features = image_embeds.shape[0] #number of image features: 1190. n_image_features==n_image_tokens or raise ValueError\n",
    "print('[n_image_tokens] ',n_image_tokens,' [n_image_features] ',n_image_features)\n",
    "\n",
    "image_mask = (\n",
    "    # output bool tensor, in which `True` is input_ids' value==image_token_id, and output shape==input_ids.shape\n",
    "    (input_ids == model.config.image_token_id)\n",
    "    .unsqueeze(-1) # expand dimension to 1 at pos -1\n",
    "    # original: [1,1220,1], after `expand_as`: [1,1220,1536]. Copies value [1,1220,0] to [1,1220,1536]\n",
    "    .expand_as(inputs_embeds)\n",
    "    .to(inputs_embeds.device)\n",
    ")\n",
    "temp1=(input_ids == model.config.image_token_id)\n",
    "print(\"[temp1 shape] \", temp1.shape, \"\\n[temp1[0,-15:]] \",temp1[0,-15:])\n",
    "temp2=temp1.unsqueeze(-1).expand_as(inputs_embeds)\n",
    "print(\"[temp2 shape] \",temp2.shape, \"\\n[temp2[0,-15:,0]] \",temp2[0,-15:,0])\n",
    "print('[image_mask shape] ',image_mask.shape)\n",
    "\n",
    "print(inputs_embeds[0,-15:,0])\n",
    "# inputs_embeds.shape == image_mask.shape, image_embeds have values >= `True` value in image_mask\n",
    "# replace inputs_embeds's value where image_mask==True\n",
    "inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "print(inputs_embeds[0,-15:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below is languange model, Qwen2VLModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inputs_embeds shape]  torch.Size([1, 1220, 1536])\n",
      "[position_ids shape]  torch.Size([3, 1, 1220])\n",
      "[position_ids]\n",
      " tensor([[[   0,    1,    2,  ..., 1217, 1218, 1219]],\n",
      "\n",
      "        [[   0,    1,    2,  ..., 1217, 1218, 1219]],\n",
      "\n",
      "        [[   0,    1,    2,  ..., 1217, 1218, 1219]]], device='cuda:0')\n",
      "[causal_mask shape]  torch.Size([1, 1, 1220, 1221])\n",
      "[causal_mask]\n",
      " tensor([[[[-0.0000e+00, -3.3895e+38, -3.3895e+38,  ..., -3.3895e+38,\n",
      "           -3.3895e+38, -3.3895e+38],\n",
      "          [-0.0000e+00, -0.0000e+00, -3.3895e+38,  ..., -3.3895e+38,\n",
      "           -3.3895e+38, -3.3895e+38],\n",
      "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.3895e+38,\n",
      "           -3.3895e+38, -3.3895e+38],\n",
      "          ...,\n",
      "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.3895e+38,\n",
      "           -3.3895e+38, -3.3895e+38],\n",
      "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "           -3.3895e+38, -3.3895e+38],\n",
      "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "           -0.0000e+00, -3.3895e+38]]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "[position_embeddings cos shape]  torch.Size([3, 1, 1220, 128])\n",
      "[position_embeddings sin shape==cos shape]\n",
      "[cos[0,0,:50,:]]\n",
      " tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5391,  0.6914,  0.7969,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.4160, -0.0408,  0.2695,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        ...,\n",
      "        [-0.9922,  0.9844,  0.6250,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.6406,  0.5547,  0.9688,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.3008, -0.2148,  0.9180,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "[cos[1,0,:50,:]]\n",
      " tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5391,  0.6914,  0.7969,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.4160, -0.0408,  0.2695,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        ...,\n",
      "        [-0.9922,  0.9844,  0.6250,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.6406,  0.5547,  0.9688,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.3008, -0.2148,  0.9180,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "[cos[2,0,:50,:]]\n",
      " tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5391,  0.6914,  0.7969,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.4160, -0.0408,  0.2695,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        ...,\n",
      "        [-0.9922,  0.9844,  0.6250,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.6406,  0.5547,  0.9688,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.3008, -0.2148,  0.9180,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"analyze Qwen2VL.Qwen2VLModel.forward\"\"\"\n",
    "\n",
    "print(\"[inputs_embeds shape] \",inputs_embeds.shape)\n",
    "past_seen_tokens=0\n",
    "cache_position = torch.arange(\n",
    "    past_seen_tokens,\n",
    "    past_seen_tokens + inputs_embeds.shape[1],\n",
    "    device=inputs_embeds.device\n",
    ") # tensor([0,1,2,...,1219]) shape==[1220]\n",
    "position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n",
    "print(\"[position_ids shape] \",position_ids.shape)\n",
    "print(\"[position_ids]\\n\",position_ids)\n",
    "\n",
    "# tri matrix, lower is 0, upper is min of bfloat16: [0\\min]\n",
    "causal_mask=model.model._update_causal_mask(\n",
    "    None,inputs_embeds,cache_position,None,None)\n",
    "print(\"[causal_mask shape] \",causal_mask.shape)\n",
    "print(\"[causal_mask]\\n\",causal_mask)\n",
    "attention_mask=causal_mask\n",
    "\n",
    "# Difference between rotary_emb here and VisionModel.rotary_emb is only the first dim\n",
    "# rotary_emb here repeat 3 times at the first dim.\n",
    "position_embeddings=model.model.rotary_emb(inputs_embeds,position_ids)\n",
    "cos,sin=position_embeddings\n",
    "print(\"[position_embeddings cos shape] \",cos.shape)\n",
    "print(\"[position_embeddings sin shape==cos shape]\")\n",
    "print(\"[cos[0,0,:50,:]]\\n\",cos[0,0,:50,:])\n",
    "print(\"[cos[1,0,:50,:]]\\n\",cos[1,0,:50,:])\n",
    "print(\"[cos[2,0,:50,:]]\\n\",cos[2,0,:50,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[query_states shape]  torch.Size([1, 12, 1220, 128])\n",
      "[key_states shape]  torch.Size([1, 2, 1220, 128])\n",
      "[value_states.shape==key_states.shape]\n",
      "[cos_after_cat shape]  torch.Size([1, 1, 1220, 128])\n",
      "[sin_after_cat.shape==cos_after_cat.shape]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"analyze Qwen2VLAttention\"\"\"\n",
    "llm_hidden_states=inputs_embeds\n",
    "attention_mask=attention_mask\n",
    "position_ids=position_ids\n",
    "position_embeddings=position_embeddings\n",
    "llm_self_attn=model.model.layers[0].self_attn\n",
    "\n",
    "num_heads=llm_self_attn.num_heads\n",
    "num_key_value_heads=llm_self_attn.num_key_value_heads\n",
    "head_dim=llm_self_attn.head_dim\n",
    "\n",
    "q_proj=llm_self_attn.q_proj\n",
    "k_proj=llm_self_attn.k_proj\n",
    "v_proj=llm_self_attn.v_proj\n",
    "\n",
    "bsz, q_len, _ = llm_hidden_states.size()\n",
    "\n",
    "query_states = q_proj(llm_hidden_states)\n",
    "key_states = k_proj(llm_hidden_states)\n",
    "value_states = v_proj(llm_hidden_states)\n",
    "\n",
    "query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "key_states = key_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "# be aware of transpose(1,2). [bsz, num_heads, q_len, head_dim]\n",
    "print(\"[query_states shape] \",query_states.shape)\n",
    "print(\"[key_states shape] \",key_states.shape)\n",
    "print(\"[value_states.shape==key_states.shape]\")\n",
    "kv_seq_len = key_states.shape[-2]\n",
    "\n",
    "### apply_multimodal_rotary_pos_emb ###\n",
    "# Copied from transformers.models.llama.modeling_llama.rotate_half\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "unsqueeze_dim=1\n",
    "cos,sin=position_embeddings # cos.shape=[3, 1, 1220, 128]==sin.shape\n",
    "mrope_section=[16,24,24]\n",
    "mrope_section=mrope_section*2 # [16,24,24,16,24,24]\n",
    "\n",
    "cos = torch.cat(\n",
    "    [m[i % 3] for i, m in enumerate(cos.split(mrope_section, dim=-1))], dim=-1\n",
    ").unsqueeze(unsqueeze_dim)\n",
    "\n",
    "sin = torch.cat(\n",
    "    [m[i % 3] for i, m in enumerate(sin.split(mrope_section, dim=-1))], dim=-1\n",
    ").unsqueeze(unsqueeze_dim)\n",
    "# 详解见notebooks下一个块\n",
    "print(\"[cos_after_cat shape] \",cos.shape)\n",
    "print(\"[sin_after_cat.shape==cos_after_cat.shape]\")\n",
    "\n",
    "q_embed = (query_states * cos) + (rotate_half(query_states) * sin)\n",
    "k_embed = (key_states * cos) + (rotate_half(key_states) * sin)\n",
    "### apply_multimodal_rotary_pos_emb ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temp_cos.shape]  torch.Size([3, 1, 10, 128])\n",
      "torch.Size([1, 10, 16])\n",
      "torch.Size([1, 10, 24])\n",
      "torch.Size([1, 10, 24])\n",
      "torch.Size([1, 10, 16])\n",
      "torch.Size([1, 10, 24])\n",
      "torch.Size([1, 10, 24])\n",
      "[after concat, temp_cos.shape]  torch.Size([1, 1, 10, 128])\n",
      "\n",
      "结论：mrope就是先expand出3个维度：time,height,width的position_ids的cos和sin\n",
      "（注：这三个维度的cos，sin完全相同，postion_ids都是从0开始到seq_len），\n",
      "然后在每个head_dim位置上嵌入不同维度的位置信息：\n",
      "即某位置特征的head_dim[16:time, 24:height, 24:width, 16:time, 24:height, 24:width]。\n",
      "在`Q*K^T`时，Q的m位置与K的n位置相乘时，不同维度的信息会与对应维度相乘。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"temp analyze cat&split in apply_multimodal_rotary_pos_emb\"\"\"\n",
    "import torch\n",
    "bsz=1\n",
    "seq_len=10\n",
    "head_dim=128\n",
    "mrope_section=[16,24,24,16,24,24]\n",
    "unsqueeze_dim=1\n",
    "temp_cos=torch.arange(bsz*seq_len*head_dim).reshape(1,bsz,seq_len,head_dim).expand(3,bsz,seq_len,head_dim)\n",
    "print(\"[temp_cos.shape] \",temp_cos.shape)\n",
    "temp_sections=[]\n",
    "# temp_cos.split(mrope_section,dim=-1): 6 tensors:\n",
    "# 0=[3, 1, seq_len, 16], 1=[3, 1, seq_len, 24], 2=[3, 1, seq_len, 24],\n",
    "# 3=[3, 1, seq_len, 16], 4=[3, 1, seq_len, 24], 5=[3, 1, seq_len, 24]\n",
    " \n",
    "# m[ i%3 ]:\n",
    "# 取m_0=[3, 1, seq_len, 16]的[0,:]的向量， (time)\n",
    "# 取m_1=[3, 1, seq_len, 24]的[1,:]的向量， (height)\n",
    "# 取m_2=[3, 1, seq_len, 24]的[2,:]的向量， (width)\n",
    "# 取m_3=[3, 1, seq_len, 16]的[0,:]的向量， (time)\n",
    "# 取m_4=[3, 1, seq_len, 24]的[1,:]的向量， (height)\n",
    "# 取m_5=[3, 1, seq_len, 24]的[2,:]的向量， (width)\n",
    "for i,m in enumerate(temp_cos.split(mrope_section,dim=-1)):\n",
    "    temp_sections.append(m[i%3])\n",
    "    # print(m[i%3])\n",
    "    print(m[i%3].shape)\n",
    "temp_cos=torch.concat(temp_sections,dim=-1).unsqueeze(unsqueeze_dim)\n",
    "print(\"[after concat, temp_cos.shape] \",temp_cos.shape)\n",
    "print(\"\"\"\n",
    "结论：mrope就是先expand出3个维度：time,height,width的position_ids的cos和sin\n",
    "（注：这三个维度的cos，sin完全相同，postion_ids都是从0开始到seq_len），\n",
    "然后在每个head_dim位置上嵌入不同维度的位置信息：\n",
    "即某位置特征的head_dim[16:time, 24:height, 24:width, 16:time, 24:height, 24:width]。\n",
    "在`Q*K^T`时，Q的m位置与K的n位置相乘时，不同维度的信息会与对应维度相乘。\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[final_hidden_states.shape]  torch.Size([1, 1220, 1536])\n",
      "[logits.shape]  torch.Size([1, 1220, 151936])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Qwen2VL.language model\"\"\"\n",
    "llm_model=model.model\n",
    "position_ids=None\n",
    "attention_mask=None\n",
    "past_key_values=None\n",
    "use_cache=False\n",
    "output_attentions=None\n",
    "output_hidden_states=None\n",
    "return_dict=None\n",
    "\n",
    "outputs = llm_model(\n",
    "    input_ids=None,\n",
    "    position_ids=position_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    past_key_values=past_key_values,\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    use_cache=use_cache,\n",
    "    output_attentions=output_attentions,\n",
    "    output_hidden_states=output_hidden_states,\n",
    "    return_dict=return_dict,\n",
    ")\n",
    "final_hidden_states=outputs[0]\n",
    "print(\"[final_hidden_states.shape] \",final_hidden_states.shape)\n",
    "\n",
    "### lm_head ###\n",
    "lm_head = model.lm_head\n",
    "logits=lm_head(final_hidden_states)\n",
    "print(\"[logits.shape] \",logits.shape)\n",
    "### lm_head ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generation of the output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
